[
["index.html", "机器学习笔记 第 1 章 简介", " 机器学习笔记 2018-12-16 第 1 章 简介 "],
["lineregression.html", "第 2 章 线性回归", " 第 2 章 线性回归 线性回归中关于误差项 \\(\\epsilon\\) 的假设,也称为 Gauss- Markov 假设: 误差项均值为0 等方差 误差彼此之间是不相关的 "],
["classification.html", "第 3 章 分类 3.1 逻辑回归 3.2 朴素贝叶斯 3.3 决策树 3.4 集成学习 3.5 支持量机", " 第 3 章 分类 3.1 逻辑回归 线性回归在量度上没有加以限制, 可以取连续值，也可以取离散值. 只是要求: 每一个自变量与其它变量线性无关； 自变量与误差项线性无关。 但是作为自变量却必须是连续值，它的取值在负无穷与正无穷之间。 对于分类任务,线性回归就不能胜任了，尤其对于二分类问题。 3.1.1 原理 3.1.2 参数估计 3.1.2.1 最大似然估计 利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值(模型已知，参数未知） 当从模型总体随机抽取n组样本观测值后，最合理的参数估计量应该使得从模型中抽取该n组样本观测值的概率最大，而不是像最小二乘估计法旨在得到使得模型能最好地拟合样本数据的参数估计量。 梯度下降法 3.1.3 和线性回归的区别 无需事先假设数据的分布,这样避免了假设分布不准确所带来的问题; 不仅输出了分类的类型,还输出了各个类别的概率。 3.2 朴素贝叶斯 朴素贝叶斯是基于贝叶斯定理和特征条件独立假设的分类方法。对于输入训练集，基于特征条件独立的假设条件，来学习输入与输出之间的联合概率分布，基于此模型，对于新输入的样本，通过贝叶斯定理来计算使得后验概率最大的 \\(y\\) 值。 3.3 决策树 3.4 集成学习 通过构建并结合基学习器来学习任务，根据基学习器的生成方式，目前的集成学习方法可分为两种： 个体学习器之间存在强依赖关系，即串行化生成序列的过程，主要代表为 boosting； 个体之间不存在强依赖关系，即并行化生成序列的过程，主要代表为装袋法。 3.4.1 Boosting 工作机制 赋予相同的权重给每一个样本，并训练初始的基分类器； 根据基分类器的表现对训练样本的分布进行调整，使得先前基学习器分错的样本在后续中受到更多的关注，减少分对样本的权重，基于调整后的样本分布来训练下一个基学习器； 如此重复上面的步骤，直至学习器数目达到事先指定的值，最终将所有训练的基分类器进行加权结合。 优缺点 优点：相比较单一的分类器，误差率有所降低。 缺点：可解释性差， 过于关注降低偏差，容易发生过拟合。 3.4.2 装袋法 工作机制 采用 boostrap（自助法）从训练数据集中重复抽取样本，因此每一个样本抽取的概率是相同的。 根据上面抽取的样本训练分类或者预测模型，记录每一个预测结果； 针对步骤 2 中生成的分类模型采取大多数投票的方式进行决策，对于预测型的模型采取预测结果的平均值进行决策。 优缺点 优点：相比较单一的分类器，误差率有所降低。 缺点：可解释性差， 更加关注模型的泛化性。 3.4.3 随机森林 3.4.4 XGboost 3.5 支持量机 "],
["cluster.html", "第 4 章 聚类 4.1 K-Means 4.2 K-Means++", " 第 4 章 聚类 根据训练样本中是否包含标签信息，机器学习分为监督学习和无监督学习，聚类算法是典型的无监督学习，其训练样本中只包含样本特征，不包含样本的标签信息。在聚类算法中，利用样本的特征，将具有相似属性的样本划分到同一个类别中。 4.1 K-Means 随机选取 \\(k\\) （预设类别数）个样本作为起始中心点，将其余样本归入相似度较高中心点所在的簇中，在确立当前簇中的样本的均值作为新的中心点，依次循环迭代下去，直至所有的样本所属的类别不在发生变化。如下图 4.11 所示: 图 4.1: k均值示意图 4.2 K-Means++ K-Means 中首先需要确定聚类的个数 \\(k\\) , 这一点对于未知数据具有较大的局限性; 其次在利用 K-Means 算法进行聚类之前，需要初始化 \\(k\\) 个聚类中心， 如果初始化聚类中心选择不好的话，将会对最终的聚类效果造成较大的影响。为了解决初始化问题带给 K-Means 算法的问题，2007年由D.Arthur 等人提出的 K-Means++2 针对初始化中心点的选取做了改进。具体如下: 从数据集中随机选取一个样本作为初始聚类中心 \\(C_1\\); 计算每一个样本与当前聚类中心之间的最短距离 （即与最近的一个聚类中心的距离），若用 \\(D(x)\\) 表示； 计算每一个样本被选为下一个聚类中心的概率 \\(\\frac{{D(x)}^2}{\\sum_{x \\in X}{D(x)}^2}\\), 以概率选择距离最大的样本作为新的聚类中心，重复上面的重复上面的过程，直至 \\(k\\) 个聚类中心被确定下来; 其它的步骤和 K-Means 一样。 黄文, 王正林. 数据挖掘 : R语言实战[M]. 电子工业出版社, 2014.↩ Arthur D, Vassilvitskii S. k-means++:the advantages of careful seeding[C] Eighteenth Acm-Siam Symposium on Discrete Algorithms, New Orleans, Louisiana. Society for Industrial and Applied Mathematics, 2007:1027-1035.↩ "],
["basic.html", "A 基础知识", " A 基础知识 "]
]
