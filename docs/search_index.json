[
["index.html", "机器学习笔记 第 1 章 简介", " 机器学习笔记 2018-06-19 第 1 章 简介 "],
["lineregression.html", "第 2 章 线性回归", " 第 2 章 线性回归 线性回归中关于误差项 \\(\\epsilon\\) 的假设,也称为 Gauss- Markov 假设: 误差项均值为0 等方差 误差彼此之间是不相关的 "],
["classification.html", "第 3 章 分类 3.1 逻辑回归", " 第 3 章 分类 3.1 逻辑回归 线性回归在量度上没有加以限制, 可以取连续值,也可以取离散值. 只是要求: 每一个自变量与其它变量线性无关; 自变量与误差项线性无关. 但是作为自变量却必须是连续值,它的取值在负无穷与正无穷之间. 对于分类任务,线性回归就不能胜任了.尤其对于二分类问题. 3.1.1 原理 3.1.2 参数估计 3.1.2.1 最大似然估计 利用已知的样本结果,反推最有可能（最大概率）导致这样结果的参数值(模型已知,参数未知） 当从模型总体随机抽取n组样本观测值后，最合理的参数估计量应该使得从模型中抽取该n组样本观测值的概率最大，而不是像最小二乘估计法旨在得到使得模型能最好地拟合样本数据的参数估计量 梯度下降法 3.1.3 和线性回归的区别 无需事先假设数据的分布,这样避免了假设分布不准确所带来的问题; 不仅输出了分类的类型,还输出了各个类别的概率. "],
["cluster.html", "第 4 章 聚类 4.1 K-Means 4.2 K-Means++", " 第 4 章 聚类 根据训练样中是否包含标签信息，机器学习分为监督学习和无监督学习，聚类算法是典型的无监督学习，其训练样本中只包含样本特征，不包含样本的标签信息。在聚类算法中，利用样本的特征，将具有相似属性的样本划分到同一个类别中。 4.1 K-Means 随机选取 \\(k\\) （预设类别数）个样本作为其实中心点，将其余样本归入相似度较高中心点所在的簇中，在确立当前簇中的样本的均值作为新的中心点，依次循环迭代下去，直至所有的样本所属的类别不在发生变化。如下图 4.11 所示: 图 4.1: k均值示意图 4.2 K-Means++ K-Means 中首先需要确定聚类的个数 \\(k\\) , 这一点对于未知数据具有较大的局限性; 其次在利用 K-Means 算法进行聚类之前，需要初始化 \\(k\\) 个聚类中心， 如果初始化聚类中心选择不好的话，将会最终的聚类效果造成较大的影响。为了解决初始化问题带给 K-Means 算法的问题，2007年由D.Arthur 等人提出的 K-Means++2 针对初始化中心点的选取做了改进。具体如下: 从数据集中随机选取一个样本作为初始聚类中心 \\(C_1\\); 计算每一个样本与当前聚类中心之间的最短距离 （即与最近的一个聚类中心的距离），若用 \\(D(x)\\) 表示； 计算每一个样本被选为下一个聚类中心的概率 \\(\\frac{{D(x)}^2}{\\sum_{x \\in X}{D(x)}^2}\\), 以概率选择距离最大的样本作为新的聚类中心，重复上面的重复上面的过程，直至 \\(k\\) 个聚类中心被确定下来; 其它的步骤和 K-Means 一样。 黄文, 王正林. 数据挖掘 : R语言实战[M]. 电子工业出版社, 2014.↩ Arthur D, Vassilvitskii S. k-means++:the advantages of careful seeding[C] Eighteenth Acm-Siam Symposium on Discrete Algorithms, New Orleans, Louisiana. Society for Industrial and Applied Mathematics, 2007:1027-1035.↩ "],
["basic.html", "A 基础知识", " A 基础知识 "]
]
