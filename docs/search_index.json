[
["classification.html", "第 3 章 分类 3.1 逻辑回归 3.2 集成学习", " 第 3 章 分类 3.1 逻辑回归 线性回归在量度上没有加以限制, 可以取连续值，也可以取离散值. 只是要求: 每一个自变量与其它变量线性无关； 自变量与误差项线性无关。 但是作为自变量却必须是连续值，它的取值在负无穷与正无穷之间。 对于分类任务,线性回归就不能胜任了，尤其对于二分类问题。 3.1.1 原理 3.1.2 参数估计 3.1.2.1 最大似然估计 利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值(模型已知，参数未知） 当从模型总体随机抽取n组样本观测值后，最合理的参数估计量应该使得从模型中抽取该n组样本观测值的概率最大，而不是像最小二乘估计法旨在得到使得模型能最好地拟合样本数据的参数估计量。 梯度下降法 3.1.3 和线性回归的区别 无需事先假设数据的分布,这样避免了假设分布不准确所带来的问题; 不仅输出了分类的类型,还输出了各个类别的概率。 3.2 集成学习 通过构建并结合基学习器来学习任务，根据基学习器的生成方式，目前的集成学习方法可分为两种： 个体学习器之间存在强依赖关系，即串行化生成序列的过程，主要代表为 boosting； 个体之间不存在强依赖关系，即并行化生成序列的过程，主要代表为装袋法。 3.2.1 Boosting 工作机制 赋予相同的权重给每一个样本，并训练初始的基分类器； 根据基分类器的表现对训练样本的分布进行调整，使得先前基学习器分错的样本在后续中受到更多的关注，减少分对样本的权重，基于调整后的样本分布来训练下一个基学习器； 如此重复上面的步骤，直至学习器数目达到事先指定的值，最终将所有训练的基分类器进行加权结合。 优缺点 优点：相比较单一的分类器，误差率有所降低。 缺点：可解释性差， 过于关注降低偏差，容易发生过拟合。 3.2.2 装袋法 工作机制 采用 boostrap（自助法）从训练数据集中重复抽取样本，因此每一个样本抽取的概率是相同的。 根据上面抽取的样本训练分类或者预测模型，记录每一个预测结果； 针对步骤 2 中生成的分类模型采取大多数投票的方式进行决策，对于预测型的模型采取预测结果的平均值进行决策。 优缺点 优点：相比较单一的分类器，误差率有所降低。 缺点：可解释性差， 更加关注模型的泛化性。 3.2.3 随机森林 "]
]
